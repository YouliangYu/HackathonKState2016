i
head(mtrain)
i
lambda <-10^seq(-8,-1,0.1)
registerDoMC(cores=7)
i=i+1
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="mse", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-0.05,0.1)
registerDoMC(cores=7)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="mse", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-0.05,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="mse", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-2,0.01)
lambda <-10^seq(-6,-2,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="mse", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-2,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-4,-2.5,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-5,-2,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-10,-2,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-1,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-6,-1.5,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-3,-1.5,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-3,-1.5,0.002)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
i
lambda <-10^seq(-3,-1.5,0.005)
registerDoMC(cores=7)
for (j in 1:nfolds){
indexTrain <- which(xfolds$fold5 != j)
indexValid <- which(xfolds$fold5 == j)
clf <- cv.glmnet(y=y[indexTrain], x=as.matrix(train[indexTrain,]),
family="binomial",type.measure="auc", nfolds =4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
pred <- predict(clf,newx=as.matrix(train[indexValid,]), s="lambda.min",type="response")
mtrain[indexValid,i] <- pred
cat(rmse(y[indexValid],mtrain[indexValid,i]))
}
rmse(y,mtrain[,i])
head(mtrain)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="mse", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 5,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
cat(rmse(y,mtrain[,i]))
clf <- cv.glmnet(y=y,x=as.matrix(mtrain),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
clf
i
head(mtrain,2)
head(mtest,2)
clf <- cv.glmnet(y=y,x=as.matrix(train),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
clf$lambda
clf$lambda.min
lambda <-10^seq(-3,-1.5,0.005)
clf <- cv.glmnet(y=y,x=as.matrix(train),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
i
mtest[,i] <- predict(clf,newx=as.matrix(test), s="lambda.min",type="response")
head(mtrain,2)
cor(mtrain[,1:6])
cor(mtest[,1:6])
lambda <-10^seq(-6,-2,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain[,1:6]),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-5,-1,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain[,1:6]),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
lambda <-10^seq(-5,-1.5,0.01)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain[,1:6]),
family="binomial",type.measure="auc", nfolds = 4,parallel=TRUE,lambda=lambda)
qplot(clf$lambda,clf$cvm)
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'rmse',
nthread = 6,eta = 0.1, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'auc',
nthread = 6,eta = 0.1, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'auc',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'auc',
nthread = 6,eta = 0.005, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'logloss',
nthread = 6,eta = 0.005, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
clf <- cv.glmnet(y=y,x=as.matrix(mtrain[,1:6]),
family="binomial",type.measure="logloss", nfolds = 4,parallel=TRUE,lambda=lambda)
apply(mtrain[,1:5], 2, function(x) rmse(y,x))
apply(mtrain[,1:6], 2, function(x) logloss(y,x))
apply(mtrain[,1:6], 2, function(x) logLoss(y,x))
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'logloss',
nthread = 6,eta = 0.005, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
apply(mtrain[,1:6], 2, function(x) logLoss(y,x))
apply(mtrain[,1:6], 2, function(x) auc(y,x))
auc(mtrain[,1:6])
auc(y,mtrain[,1:6])
cor(mtrain[,1:6])
dtrain <- xgb.DMatrix(as.matrix(mtrain[,1:6]), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'auc',
nthread = 6,eta = 0.005, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 20)
pred <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;auc(y,pred)
head(mtrain)
head(mtrain)
save.image("~/computing/hackingkstate/challenge1/HospitalReadmissionsPredicting/stacking1.RData")
head(mtrain,2)
head(mtest,2)
apply(mtrain[,1:6], 2, function(x) auc(y,x))
apply(mtrain[,1:6], 2, function(x) auc(y,x)) #auc
apply(mtrain[,1:6], 2, function(x) rmse(y,x)) #auc
apply(mtrain[,1:6], 2, function(x) error(y,x)) #auc
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
mead(y)
mean(y)
auc(y,mean(y))
mse(y,mean(y))
logLoss(y,mean(y))
mse(y,mean(y))
pred <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred)
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
pred <- (2*mtrain[,1]+1*mtrain[,3]+3*mtrain[,4])/6;mse(y,pred)
pred <- (2*mtrain[,1]+1*mtrain[,3]+2*mtrain[,4])/6;mse(y,pred)
pred <- (2*mtrain[,1]+1*mtrain[,3]+2*mtrain[,4])/5;mse(y,pred)
pred <- (2*mtrain[,1]+1*mtrain[,3]+4*mtrain[,4])/7;mse(y,pred)
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
pred <- (2*mtrain[,1]+2*mtrain[,4])/4;mse(y,pred)
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
plot(pred_train)
pred <- pred_train
pred[pred>0.8] <- 1
pred[pred<0.8] <- 1
head(pred)
mse(y,pred)
pred_train>0.8
thre <- 0.8; pred[pred_train>thre] <- 1; pred[pred<0.8] <- 0
mse(y,pred)
head(pred<0.8)
head(pred<TRUE)
head(pred>0.8)
thre <- mean(y); pred[pred_train>thre] <- 1; pred[pred<thre] <- 0
mse(y,pred)
auc(y,y)
auc(y,0)
auc(y,mean(y))
auc(y,0.5)
plot(y,pred_train)
thre <- 1-mean(y); pred[pred_train>thre] <- 1; pred[pred<thre] <- 0
mse(y,pred)
thre <- 1-mean(y); pred[pred_train>thre] <- 1; pred[pred_train<thre] <- 0
mse(y,pred)
head(y)
mean(y)
head(pred_train)
head(pred_train>0.756)
head(y)
head(as.integer(pred_train>0.756))
pred <- as.integer(pred_train>0.756)
mse(y,pred)
head(as.integer(pred_train>0.756),10)
head(y)
head(y,10)
sum(y==pred)
sum(y==pred)/nrow(pred)
sum(y==pred)
nrow(pred)
sum(y==pred)/length(pred)
sum(y==pred_train)/length(pred)
sum(abs(y-pred_train)/2)/length(pred)
sum(1-abs(y-pred_train)/2)/length(pred)
head(pred_train)
save.image("~/computing/hackingkstate/challenge1/HospitalReadmissionsPredicting/stacking2.RData")
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('data/Challenge_1_Testing.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
# factorize char vars
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('testing.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
getwd
getwd()
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0
head(train)
head(test)
train[is.na(train)] <- 0; test[is.na(test)] <- 0
head(train)
head(test)
encode_vars <- names(train)[apply(rbind(train), 2, function(x) length(unique(x))<100)]
for (f in intersect(names(train),union(encode_vars,fact_vars))){
dummyfact <- as.factor(c(-100000,train[[f]],testx[[f]]))
temp <- model.matrix(~dummyfact)
k=0
for (j in 2:ncol(temp)){
colnames(temp)[j] <- paste0(f,"Dummy",k)
k <- k+1
}
train <- cbind(train,temp[2:(nrow(train)+1),-1])
test <- cbind(test,temp[(nrow(test)+2):nrow(temp),-1])
rm(temp);rm(dummyfact);gc()
}
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
# factorize char vars
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0; test[is.na(test)] <- 0
encode_vars
encode_vars <- names(train)[apply(rbind(train), 2, function(x) length(unique(x))<100)]
for (f in intersect(names(train),union(encode_vars,fact_vars))){
dummyfact <- as.factor(c(-100000,train[[f]],test[[f]]))
temp <- model.matrix(~dummyfact)
k=0
for (j in 2:ncol(temp)){
colnames(temp)[j] <- paste0(f,"Dummy",k)
k <- k+1
}
train <- cbind(train,temp[2:(nrow(train)+1),-1])
test <- cbind(test,temp[(nrow(train)+2):nrow(temp),-1])
rm(temp);rm(dummyfact);gc()
}
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
# factorize char vars
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0; test[is.na(test)] <- 0
set.seed(1229);dtrain <- xgb.DMatrix(as.matrix(train), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'rmse',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 50)
n=913;bst <- xgb.train(params=param, data=dtrain, nround = as.integer(n/0.8))
pred <- predict(bst, as.matrix(test));print(pred)
head(y)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
head(train$readmitted)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
submission <- data.frame(ID=testID, readmitted= as.numeric(format(pred,digits = 16,scientific = FALSE)))
testID <- test$patient_nbr
submission <- data.frame(ID=testID, readmitted= as.numeric(format(pred,digits = 16,scientific = FALSE)))
head(submission)
load("stacking2.RData")
head(mtrain)
head(mtest)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
1-0.1759973
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5]+1*mtrain[,6])/17;mse(y,pred_train)
pred_train <- (5*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+10*mtrain[,4]+2*mtrain[,5]+1*mtrain[,6])/20;mse(y,pred_train)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5]+1*mtrain[,6])/16;mse(y,pred_train)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
sum(1-abs(y-pred_train)/2)/length(pred)
pred_test <- (4*mtest[,1]+1*mtest[,2]+1*mtest[,3]+8*mtest[,4]+2*mtest[,5])/16
submission <- data.frame(ID=testID, readmitted= as.numeric(format(pred_test,digits = 16,scientific = FALSE)))
head(submission)
write.csv(submission, "validation_readmitted.csv", row.names=FALSE)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
load("final.RData")
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
library(caret)
library(data.table)
library(xgboost)
library(ranger)
library(Metrics)
library(doMC)
library(glmnet)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;mse(y,pred_train)
sum(1-abs(y-pred_train)/2)/length(pred)
mse(y,mean(y))
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;auc(y,pred_train)
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
testID <- test$patient_nbr
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
# factorize char vars
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0; test[is.na(test)] <- 0
#build xgb model
set.seed(1229);dtrain <- xgb.DMatrix(as.matrix(train), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'rmse',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
set.seed(1229);dtrain <- xgb.DMatrix(as.matrix(train), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'mse',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 50)
library(caret)
library(data.table)
library(xgboost)
library(Metrics)
train <- fread('data/Challenge_1_Training.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
test <- fread('test.csv',header =TRUE,stringsAsFactors = FALSE,
na.strings=c("?","No","NO","None",""),data.table= FALSE)
train <- train[!is.na(train$readmitted),] #rm label NA
y <- as.integer(as.factor(train$readmitted))-1
train <- train[,!names(train) %in% c("encounter_id","patient_nbr","readmitted")] #remove IDs
testID <- test$patient_nbr
test <- test[,!names(test) %in% c("encounter_id","patient_nbr")]
# factorize char vars
fact_vars <- colnames(train)[sapply(train, is.character)]
for (f in fact_vars) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]] <- as.integer(factor(test[[f]], levels=levels))
}
}
#remove constant features
cst_vars <- names(train)[apply(train, 2, function(x) length(unique(x))==1)]
for (f in cst_vars){train[[f]] <- NULL; test[[f]] <- NULL}
# remove features with all NA(>99.99%)
na_var <- names(train)[apply(train, 2, function(x)  sum(is.na(x))/nrow(train)>0.9999)]
for (f in na_var){train[[f]] <- NULL; test[[f]] <- NULL}
# impute NA
train[is.na(train)] <- 0; test[is.na(test)] <- 0
#build xgb model
set.seed(1229);dtrain <- xgb.DMatrix(as.matrix(train), label = y)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'mse',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 50)
param <- list(booster ="gbtree", objective = 'binary:logistic', eval_metric = 'rmse',
nthread = 6,eta = 0.01, colsample_bytree = 0.4, subsample = 0.8, max_depth = 6, min_child_weight=11)
bst.cv = xgb.cv(param=param, data = dtrain, nfold = 5, nrounds = 5000, early.stop.round = 50)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;auc(y,pred_train)
load("final.RData")
apply(mtrain[,1:6], 2, function(x) mse(y,x)) #auc
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16;auc(y,pred_train)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16#;auc(y,pred_train)
sum(1-abs(y-pred_train)/2)/length(pred)
pred_train <- (4*mtrain[,1]+1*mtrain[,2]+1*mtrain[,3]+8*mtrain[,4]+2*mtrain[,5])/16#;auc(y,pred_train)
sum(1-abs(y-pred_train)/2)/length(pred)
